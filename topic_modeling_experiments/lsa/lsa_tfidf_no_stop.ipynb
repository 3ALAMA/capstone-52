{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/capstone-52/Pickled_from_mongo\n"
     ]
    }
   ],
   "source": [
    "cd ../../Pickled_from_mongo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_eg_gulf_1k_sample.p  combined_eg_gulf_200k_sample.p\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../Pickled_from_mongo/combined_eg_gulf_200k_sample.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192936, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['cleaned_geo','cleaned_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.set_index(['cleaned_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 192936 entries, 0 to 95683\n",
      "Data columns (total 2 columns):\n",
      "cleaned_text    192936 non-null object\n",
      "class           192936 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192936, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27332</th>\n",
       "      <td>لما ربنا يكون عايز يعوضك يبعتلك حد زي دودي كدا مصدر سعاده وبس اقسم بالله</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3560</th>\n",
       "      <td>طيب ممكن ينده أصل ماعندوش رصيد على الفون</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50074</th>\n",
       "      <td>ويِفوت العُمر لو استنِّيت</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48122</th>\n",
       "      <td>العهد في تصريحات له اليوم مشكلة قطر صغيرة جداً جداً جداً، وحرب اليمن ستستمر لمنع تحول الحوثيين لـ\"حزب الله\" آخر على حدو…</td>\n",
       "      <td>GULF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39512</th>\n",
       "      <td>ياستي دول مليون ونص</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21638</th>\n",
       "      <td>فيه واحد ادّا شنطه للسواق يوصلها لمكان ما يعني المهم ان الشنطه دي جنبي و انا مرعوبه يكون فيها قنبله و ببصلها بصه مر…</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58370</th>\n",
       "      <td>ي كثرهم</td>\n",
       "      <td>GULF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87264</th>\n",
       "      <td>اول مره تصلي و بعده سرقو نعالك</td>\n",
       "      <td>GULF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   cleaned_text  \\\n",
       "27332  لما ربنا يكون عايز يعوضك يبعتلك حد زي دودي كدا مصدر سعاده وبس اقسم بالله                                                   \n",
       "3560   طيب ممكن ينده أصل ماعندوش رصيد على الفون                                                                                   \n",
       "50074  ويِفوت العُمر لو استنِّيت                                                                                                  \n",
       "48122  العهد في تصريحات له اليوم مشكلة قطر صغيرة جداً جداً جداً، وحرب اليمن ستستمر لمنع تحول الحوثيين لـ\"حزب الله\" آخر على حدو…   \n",
       "39512  ياستي دول مليون ونص                                                                                                        \n",
       "21638  فيه واحد ادّا شنطه للسواق يوصلها لمكان ما يعني المهم ان الشنطه دي جنبي و انا مرعوبه يكون فيها قنبله و ببصلها بصه مر…       \n",
       "58370  ي كثرهم                                                                                                                    \n",
       "87264  اول مره تصلي و بعده سرقو نعالك                                                                                             \n",
       "\n",
       "      class  \n",
       "27332  EG    \n",
       "3560   EG    \n",
       "50074  EG    \n",
       "48122  GULF  \n",
       "39512  EG    \n",
       "21638  EG    \n",
       "58370  GULF  \n",
       "87264  GULF  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark LSA with no stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode the Categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['class_numerical'] = le.fit_transform(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192936, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>class</th>\n",
       "      <th>class_numerical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54873</th>\n",
       "      <td>البعد مانزل ولا زاد قدري لكنه وجع قلب واستنزف شعور</td>\n",
       "      <td>GULF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54589</th>\n",
       "      <td>الناس تكبر وتعقل وانا اكبر واتجنن اكتر</td>\n",
       "      <td>EG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60676</th>\n",
       "      <td>لبى الرياض ️</td>\n",
       "      <td>GULF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173</th>\n",
       "      <td>لان باب الحديث لاهي بالاكل</td>\n",
       "      <td>GULF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54236</th>\n",
       "      <td>علاقات كتير نهايتها تتلخص ف جملة قالها عدوية زمان \" و جينا نبعد، قالولنا نقعد و جينا نقعد، شدوا الكراسي \"</td>\n",
       "      <td>EG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26591</th>\n",
       "      <td>ايوا كدا ف السليم</td>\n",
       "      <td>EG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88116</th>\n",
       "      <td>العشم وحش</td>\n",
       "      <td>EG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79943</th>\n",
       "      <td>لا يمكنني إيقاف قلبي عن الإ اشتياق إليك ولا يمكنني منع قلبي من حبك يانور عيوني ونبضات قلبي العربي</td>\n",
       "      <td>GULF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    cleaned_text  \\\n",
       "54873  البعد مانزل ولا زاد قدري لكنه وجع قلب واستنزف شعور                                                          \n",
       "54589  الناس تكبر وتعقل وانا اكبر واتجنن اكتر                                                                      \n",
       "60676  لبى الرياض ️                                                                                                \n",
       "8173   لان باب الحديث لاهي بالاكل                                                                                  \n",
       "54236  علاقات كتير نهايتها تتلخص ف جملة قالها عدوية زمان \" و جينا نبعد، قالولنا نقعد و جينا نقعد، شدوا الكراسي \"   \n",
       "26591  ايوا كدا ف السليم                                                                                           \n",
       "88116  العشم وحش                                                                                                   \n",
       "79943  لا يمكنني إيقاف قلبي عن الإ اشتياق إليك ولا يمكنني منع قلبي من حبك يانور عيوني ونبضات قلبي العربي           \n",
       "\n",
       "      class  class_numerical  \n",
       "54873  GULF  1                \n",
       "54589  EG    0                \n",
       "60676  GULF  1                \n",
       "8173   GULF  1                \n",
       "54236  EG    0                \n",
       "26591  EG    0                \n",
       "88116  EG    0                \n",
       "79943  GULF  1                "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.class_numerical.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Document Term Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix_sps = tfidf_vectorizer.fit_transform(df.cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<192936x223045 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1807963 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix_sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_array = np.asarray(document_term_matrix_sps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<192936x223045 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1807963 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-023f9b16a878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtm_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_term_matrix_sps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mtodense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \"\"\"\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;31m##############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dtm_dense = document_term_matrix_sps.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-6a6e2c5a992c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtm_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "dtm_array[:,1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-9c7574058139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtm_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "dtm_array[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(tfidf * tfidf.T).A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<192936x223045 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1807963 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix_sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__',\n",
       " '¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼⅓¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼',\n",
       " 'éé',\n",
       " 'ąšşåň',\n",
       " 'ąℓï',\n",
       " 'ışı',\n",
       " 'ĸя',\n",
       " 'ńò',\n",
       " 'əŕ',\n",
       " 'ɹɹɹ',\n",
       " 'глаз',\n",
       " 'днем',\n",
       " 'из',\n",
       " 'моих',\n",
       " 'рождения',\n",
       " 'яαωαα',\n",
       " 'օℓℓօω',\n",
       " 'בــرام',\n",
       " 'בـوآلـي',\n",
       " 'בـﻤـﺪﻩ',\n",
       " 'וلي',\n",
       " 'ךצשךטל',\n",
       " 'םבםב',\n",
       " 'ءֆ',\n",
       " 'ءֆء',\n",
       " 'ءء',\n",
       " 'ءءءءءءءء',\n",
       " 'ءاخر',\n",
       " 'ءالتي',\n",
       " 'ءالله',\n",
       " 'ءدم',\n",
       " 'ءناس',\n",
       " 'ءوف',\n",
       " 'ءﺍﻣﻨﻮا',\n",
       " 'آء',\n",
       " 'آءة',\n",
       " 'آءت',\n",
       " 'آآ',\n",
       " 'آآء',\n",
       " 'آآآ',\n",
       " 'آآآآآح',\n",
       " 'آآآآح',\n",
       " 'آآآآمج',\n",
       " 'آآآآگ',\n",
       " 'آآآت',\n",
       " 'آآآج',\n",
       " 'آآآح',\n",
       " 'آآآسہ',\n",
       " 'آآآني',\n",
       " 'آآآه',\n",
       " 'آآآگي',\n",
       " 'آآج',\n",
       " 'آآح',\n",
       " 'آآس',\n",
       " 'آآسسہ',\n",
       " 'آآسعد',\n",
       " 'آآللي',\n",
       " 'آآمني',\n",
       " 'آآمين',\n",
       " 'آآمييين',\n",
       " 'آآه',\n",
       " 'آآهم',\n",
       " 'آآگ',\n",
       " 'آؤ',\n",
       " 'آؤل',\n",
       " 'آئ',\n",
       " 'آئئ',\n",
       " 'آئعہ',\n",
       " 'آئف',\n",
       " 'آئمآ',\n",
       " 'آئن',\n",
       " 'آئي',\n",
       " 'آاااااااااااااااه',\n",
       " 'آاااازوله',\n",
       " 'آاب',\n",
       " 'آات',\n",
       " 'آاح',\n",
       " 'آاحن',\n",
       " 'آاش',\n",
       " 'آب',\n",
       " 'آباءنا',\n",
       " 'آباءهم',\n",
       " 'آبائكم',\n",
       " 'آبائه',\n",
       " 'آبائهم',\n",
       " 'آبارا',\n",
       " 'آبت',\n",
       " 'آبتدى',\n",
       " 'آبتسآم',\n",
       " 'آبتسم',\n",
       " 'آبدا',\n",
       " 'آبذل',\n",
       " 'آبرن',\n",
       " 'آبسألك',\n",
       " 'آبعد',\n",
       " 'آبـرتـآإح',\n",
       " 'آبقى',\n",
       " 'آبل',\n",
       " 'آبه',\n",
       " 'آبواب',\n",
       " 'آبوس',\n",
       " 'آبوعدك',\n",
       " 'آبوووو',\n",
       " 'آبووووك',\n",
       " 'آبى',\n",
       " 'آبيض',\n",
       " 'آبيك',\n",
       " 'آة',\n",
       " 'آت',\n",
       " 'آتاا',\n",
       " 'آتاري',\n",
       " 'آتانا',\n",
       " 'آتاه',\n",
       " 'آتاها',\n",
       " 'آتاهم',\n",
       " 'آترك',\n",
       " 'آتركونا',\n",
       " 'آتسو',\n",
       " 'آتصبر',\n",
       " 'آتصل',\n",
       " 'آتغير',\n",
       " 'آتغيرت',\n",
       " 'آتــنا',\n",
       " 'آتكلمت',\n",
       " 'آتلفت',\n",
       " 'آتم',\n",
       " 'آتمسك',\n",
       " 'آتمق',\n",
       " 'آتمناه',\n",
       " 'آتمنى',\n",
       " 'آتنا',\n",
       " 'آتنازل',\n",
       " 'آتنفسها',\n",
       " 'آتنين',\n",
       " 'آتوب',\n",
       " 'آتى',\n",
       " 'آتي',\n",
       " 'آتية',\n",
       " 'آتيك',\n",
       " 'آتيه',\n",
       " 'آتگ',\n",
       " 'آث',\n",
       " 'آثاار',\n",
       " 'آثار',\n",
       " 'آثاره',\n",
       " 'آثارها',\n",
       " 'آثارهم',\n",
       " 'آثر',\n",
       " 'آثرتك',\n",
       " 'آثـ',\n",
       " 'آثق',\n",
       " 'آثما',\n",
       " 'آثمن',\n",
       " 'آثمه',\n",
       " 'آثنين',\n",
       " 'آج',\n",
       " 'آجبرك',\n",
       " 'آجتمع',\n",
       " 'آججم',\n",
       " 'آجدع',\n",
       " 'آجر',\n",
       " 'آجرب',\n",
       " 'آجرلهم',\n",
       " 'آجرن',\n",
       " 'آجرنا',\n",
       " 'آجرني',\n",
       " 'آجعل',\n",
       " 'آجعلك',\n",
       " 'آجعلني',\n",
       " 'آجعلهآ',\n",
       " 'آجــر',\n",
       " 'آجـل',\n",
       " 'آجـمل',\n",
       " 'آجل',\n",
       " 'آجلا',\n",
       " 'آجم',\n",
       " 'آجمعين',\n",
       " 'آجمل',\n",
       " 'آجنآس',\n",
       " 'آجه',\n",
       " 'آجى',\n",
       " 'آجي',\n",
       " 'آجيد',\n",
       " 'آح',\n",
       " 'آحب',\n",
       " 'آحبا',\n",
       " 'آحببتهم',\n",
       " 'آحببناها',\n",
       " 'آحبك',\n",
       " 'آحبكك',\n",
       " 'آحبه',\n",
       " 'آحت',\n",
       " 'آحتجتك',\n",
       " 'آحترآمك',\n",
       " 'آحترآمہ',\n",
       " 'آحترام',\n",
       " 'آحترامك',\n",
       " 'آحترم',\n",
       " 'آحتيآجي',\n",
       " 'آحد',\n",
       " 'آحرفي',\n",
       " 'آحس',\n",
       " 'آحسآس',\n",
       " 'آحسآسہ',\n",
       " 'آحساسي',\n",
       " 'آحساك',\n",
       " 'آحسنت',\n",
       " 'آحطك',\n",
       " 'آحـبــگ',\n",
       " 'آحل',\n",
       " 'آحلآم',\n",
       " 'آحلام',\n",
       " 'آحلى',\n",
       " 'آحمد',\n",
       " 'آحن',\n",
       " 'آحنا',\n",
       " 'آحوال',\n",
       " 'آحوالك',\n",
       " 'آحيآنآ',\n",
       " 'آحيآنا',\n",
       " 'آحيان',\n",
       " 'آحيانآ',\n",
       " 'آحيانا',\n",
       " 'آحيــــآنا',\n",
       " 'آخ',\n",
       " 'آخباري',\n",
       " 'آخبي',\n",
       " 'آختآرو',\n",
       " 'آختآروا',\n",
       " 'آخترت',\n",
       " 'آختكـ',\n",
       " 'آختيآره',\n",
       " 'آخد',\n",
       " 'آخدها',\n",
       " 'آخذ',\n",
       " 'آخذإجازه',\n",
       " 'آخذمصرف',\n",
       " 'آخذه',\n",
       " 'آخر',\n",
       " 'آخرا',\n",
       " 'آخرة',\n",
       " 'آخرتك',\n",
       " 'آخرته',\n",
       " 'آخرتها',\n",
       " 'آخرتي',\n",
       " 'آخرج',\n",
       " 'آخرك',\n",
       " 'آخره',\n",
       " 'آخرها',\n",
       " 'آخرهم',\n",
       " 'آخرون',\n",
       " 'آخرى',\n",
       " 'آخري',\n",
       " 'آخرين',\n",
       " 'آخسر',\n",
       " 'آخضر',\n",
       " 'آخـت',\n",
       " 'آخـر',\n",
       " 'آخـسر',\n",
       " 'آخـض',\n",
       " 'آخـــر',\n",
       " 'آخـف',\n",
       " 'آخـل',\n",
       " 'آخـڒ',\n",
       " 'آخلآقك',\n",
       " 'آخلق',\n",
       " 'آخن',\n",
       " 'آخود',\n",
       " 'آخوياا',\n",
       " 'آخيرا',\n",
       " 'آد',\n",
       " 'آداء',\n",
       " 'آداب',\n",
       " 'آدابه',\n",
       " 'آدبهم',\n",
       " 'آدة',\n",
       " 'آدد',\n",
       " 'آدرة',\n",
       " 'آدركت',\n",
       " 'آدركو',\n",
       " 'آدري',\n",
       " 'آدعواله',\n",
       " 'آدقه',\n",
       " 'آدم',\n",
       " 'آدمنته',\n",
       " 'آدمى',\n",
       " 'آدمي',\n",
       " 'آدمية',\n",
       " 'آدمين',\n",
       " 'آدميه',\n",
       " 'آدميين',\n",
       " 'آدو',\n",
       " 'آدى',\n",
       " 'آذآ',\n",
       " 'آذا',\n",
       " 'آذار',\n",
       " 'آذاك',\n",
       " 'آذاكم',\n",
       " 'آذان',\n",
       " 'آذاننا',\n",
       " 'آذانهم',\n",
       " 'آذاني',\n",
       " 'آذاه',\n",
       " 'آذتني',\n",
       " 'آذيتني',\n",
       " 'آذيته',\n",
       " 'آذيتوا',\n",
       " 'آذيه',\n",
       " 'آر',\n",
       " 'آرآ',\n",
       " 'آرآء',\n",
       " 'آرؤع',\n",
       " 'آراء',\n",
       " 'آراءنا',\n",
       " 'آراءهم',\n",
       " 'آراؤهم',\n",
       " 'آرائك',\n",
       " 'آرائنا',\n",
       " 'آرائه',\n",
       " 'آرائهم',\n",
       " 'آراك',\n",
       " 'آراها',\n",
       " 'آرب',\n",
       " 'آرت',\n",
       " 'آرتست',\n",
       " 'آرح',\n",
       " 'آرحم',\n",
       " 'آردوغانكم',\n",
       " 'آرزقني',\n",
       " 'آرس',\n",
       " 'آرسنال',\n",
       " 'آرض',\n",
       " 'آرضآء',\n",
       " 'آرضي',\n",
       " 'آركابي',\n",
       " 'آركــد',\n",
       " 'آرهآقه',\n",
       " 'آرو',\n",
       " 'آرواح',\n",
       " 'آروح',\n",
       " 'آروع',\n",
       " 'آرولار',\n",
       " 'آرى',\n",
       " 'آري',\n",
       " 'آريد',\n",
       " 'آرڒ',\n",
       " 'آرگ',\n",
       " 'آرہ',\n",
       " 'آرہب',\n",
       " 'آرہۆآمست',\n",
       " 'آز',\n",
       " 'آزار',\n",
       " 'آزاى',\n",
       " 'آزاي',\n",
       " 'آزر',\n",
       " 'آزعجني',\n",
       " 'آزوال',\n",
       " 'آزين',\n",
       " 'آس',\n",
       " 'آسأل',\n",
       " 'آسألك',\n",
       " 'آسئله',\n",
       " 'آسافر',\n",
       " 'آست',\n",
       " 'آسترخي',\n",
       " 'آستطيع',\n",
       " 'آستغفر',\n",
       " 'آستغفرالله',\n",
       " 'آستودعتگ',\n",
       " 'آستودعنآك',\n",
       " 'آسر',\n",
       " 'آسرقني',\n",
       " 'آسرني',\n",
       " 'آسعار',\n",
       " 'آسعد',\n",
       " 'آسعدتمـ',\n",
       " 'آسعدنا',\n",
       " 'آسـآلگ',\n",
       " 'آسـتر',\n",
       " 'آسـتغفرآللهہ',\n",
       " 'آسـمـگ',\n",
       " 'آسف',\n",
       " 'آسفة',\n",
       " 'آسفه',\n",
       " 'آسفين',\n",
       " 'آسقي',\n",
       " 'آسك',\n",
       " 'آسلوبك',\n",
       " 'آسم',\n",
       " 'آسمك',\n",
       " 'آسمي',\n",
       " 'آسهره',\n",
       " 'آسود',\n",
       " 'آسي',\n",
       " 'آسيا',\n",
       " 'آسيوي',\n",
       " 'آسيوية',\n",
       " 'آش',\n",
       " 'آشت',\n",
       " 'آشتآق',\n",
       " 'آشتهي',\n",
       " 'آشخاص',\n",
       " 'آشرب',\n",
       " 'آشع',\n",
       " 'آشعر',\n",
       " 'آشــتهي',\n",
       " 'آشك',\n",
       " 'آشكره',\n",
       " 'آشلي',\n",
       " 'آشهد',\n",
       " 'آشوآق',\n",
       " 'آشوفه',\n",
       " 'آشياء',\n",
       " 'آص',\n",
       " 'آصالح',\n",
       " 'آصبحت',\n",
       " 'آصبر',\n",
       " 'آصحى',\n",
       " 'آصدقاء',\n",
       " 'آصدقائهم',\n",
       " 'آصر',\n",
       " 'آصعب',\n",
       " 'آصلا',\n",
       " 'آض',\n",
       " 'آضــع',\n",
       " 'آضي',\n",
       " 'آط',\n",
       " 'آطآلة',\n",
       " 'آطري',\n",
       " 'آطيح',\n",
       " 'آع',\n",
       " 'آعافر',\n",
       " 'آعتذآر',\n",
       " 'آعجآب',\n",
       " 'آعد',\n",
       " 'آعدك',\n",
       " 'آعرف',\n",
       " 'آعشق',\n",
       " 'آعشقه',\n",
       " 'آعشقوآ',\n",
       " 'آعطآني',\n",
       " 'آعطت',\n",
       " 'آعطني',\n",
       " 'آعطى',\n",
       " 'آعطي',\n",
       " 'آعطيتہ',\n",
       " 'آعفو',\n",
       " 'آعلاميين',\n",
       " 'آعمآق',\n",
       " 'آعمق',\n",
       " 'آعي',\n",
       " 'آعيش',\n",
       " 'آعيشه',\n",
       " 'آغ',\n",
       " 'آغا',\n",
       " 'آغار',\n",
       " 'آغاى',\n",
       " 'آغت',\n",
       " 'آغدو',\n",
       " 'آغـا',\n",
       " 'آغـــلى',\n",
       " 'آغلقتـه',\n",
       " 'آغلى',\n",
       " 'آغيب',\n",
       " 'آف',\n",
       " 'آفاتارك',\n",
       " 'آفاق',\n",
       " 'آفاقها',\n",
       " 'آفة',\n",
       " 'آفت',\n",
       " 'آفتار',\n",
       " 'آفتارك',\n",
       " 'آفتح',\n",
       " 'آفتشك',\n",
       " 'آفضـح',\n",
       " 'آفعآلك',\n",
       " 'آفـآق',\n",
       " 'آفـــرد',\n",
       " 'آفـوت',\n",
       " 'آفكين',\n",
       " 'آفنوني',\n",
       " 'آفي',\n",
       " 'آفيق',\n",
       " 'آفييق',\n",
       " 'آفيہ',\n",
       " 'آق',\n",
       " 'آقبلونا',\n",
       " 'آقتنعت',\n",
       " 'آقداري',\n",
       " 'آقدس',\n",
       " 'آقسام',\n",
       " 'آقسو',\n",
       " 'آقفيت',\n",
       " 'آقول',\n",
       " 'آقولگ',\n",
       " 'آقين',\n",
       " 'آك',\n",
       " 'آكبر',\n",
       " 'آكبرمن',\n",
       " 'آكتب',\n",
       " 'آكتبك',\n",
       " 'آكتر',\n",
       " 'آكث',\n",
       " 'آكثر',\n",
       " 'آكرمني',\n",
       " 'آكشلي',\n",
       " 'آكشن',\n",
       " 'آكل',\n",
       " 'آكله',\n",
       " 'آكلوا',\n",
       " 'آكون',\n",
       " 'آكي',\n",
       " 'آكيد',\n",
       " 'آل',\n",
       " 'آلآ',\n",
       " 'آلآة',\n",
       " 'آلآح',\n",
       " 'آلآحلآم',\n",
       " 'آلآختنآق',\n",
       " 'آلآختيآر',\n",
       " 'آلآخرة',\n",
       " 'آلآخـرين',\n",
       " 'آلآخيآر',\n",
       " 'آلآر',\n",
       " 'آلآشخآص',\n",
       " 'آلآعلى',\n",
       " 'آلآف',\n",
       " 'آلآن',\n",
       " 'آلآنتقام',\n",
       " 'آلآهتمآم',\n",
       " 'آلآوآدم',\n",
       " 'آلآورآق',\n",
       " 'آلأبيض',\n",
       " 'آلأحسآس',\n",
       " 'آلأخ',\n",
       " 'آلأخرون',\n",
       " 'آلأدب',\n",
       " 'آلأصنآم',\n",
       " 'آلألوآن',\n",
       " 'آلأولـــى',\n",
       " 'آلإحترآم',\n",
       " 'آلا',\n",
       " 'آلاء',\n",
       " 'آلات',\n",
       " 'آلاخلاق',\n",
       " 'آلاشو',\n",
       " 'آلاشياء',\n",
       " 'آلاعلام',\n",
       " 'آلاف',\n",
       " 'آلافات',\n",
       " 'آلاقي',\n",
       " 'آلام',\n",
       " 'آلامثل',\n",
       " 'آلامك',\n",
       " 'آلامل',\n",
       " 'آلامنا',\n",
       " 'آلامه',\n",
       " 'آلامها',\n",
       " 'آلامي',\n",
       " 'آلاميال',\n",
       " 'آلان',\n",
       " 'آلاوقآت',\n",
       " 'آلب',\n",
       " 'آلبآل',\n",
       " 'آلبسيطہ',\n",
       " 'آلبشر',\n",
       " 'آلبشريه',\n",
       " 'آلبشـ',\n",
       " 'آلبعض',\n",
       " 'آلبنات',\n",
       " 'آلبنت',\n",
       " 'آلبڪآء',\n",
       " 'آلة',\n",
       " 'آلت',\n",
       " 'آلترآب',\n",
       " 'آلتسآمح',\n",
       " 'آلتسامح',\n",
       " 'آلتصرفآت',\n",
       " 'آلتفآؤ',\n",
       " 'آلتمنى',\n",
       " 'آلته',\n",
       " 'آلتي',\n",
       " 'آلث',\n",
       " 'آلثقل',\n",
       " 'آلج',\n",
       " 'آلجرح',\n",
       " 'آلجـمآيل',\n",
       " 'آلجم',\n",
       " 'آلجميلہ',\n",
       " 'آلجن',\n",
       " 'آلجنه',\n",
       " 'آلجيـل',\n",
       " 'آلح',\n",
       " 'آلحآل',\n",
       " 'آلحب',\n",
       " 'آلحرام',\n",
       " 'آلحزن',\n",
       " 'آلحزين',\n",
       " 'آلحـآل',\n",
       " 'آلحــيـــآة',\n",
       " 'آلحقيقية',\n",
       " 'آلحلوين',\n",
       " 'آلحلوہ',\n",
       " 'آلحمد',\n",
       " 'آلحمدلله',\n",
       " 'آلحنين',\n",
       " 'آلحيآة',\n",
       " 'آلحيآه',\n",
       " 'آلحياة',\n",
       " 'آلحيين',\n",
       " 'آلخ',\n",
       " 'آلخآتمة',\n",
       " 'آلخآتمہ',\n",
       " 'آلختام',\n",
       " 'آلخشره',\n",
       " 'آلخـآطـر',\n",
       " 'آلخلق',\n",
       " 'آلخي',\n",
       " 'آلخيبآت',\n",
       " 'آلخير',\n",
       " 'آلخيـر',\n",
       " 'آلد',\n",
       " 'آلدم',\n",
       " 'آلدنيآ',\n",
       " 'آلدنيا',\n",
       " 'آلدهر',\n",
       " 'آلذآت',\n",
       " 'آلذكرى',\n",
       " 'آلذنوب',\n",
       " 'آلذي',\n",
       " 'آلر',\n",
       " 'آلرت',\n",
       " 'آلرجآء',\n",
       " 'آلرجاء',\n",
       " 'آلرزق',\n",
       " 'آلزين',\n",
       " 'آلس',\n",
       " 'آلسرير',\n",
       " 'آلسعآده',\n",
       " 'آلسعاده',\n",
       " 'آلسعود',\n",
       " 'آلسـعآده',\n",
       " 'آلسمآء',\n",
       " 'آلسماء',\n",
       " 'آلش',\n",
       " 'آلشتآء',\n",
       " 'آلشتويه',\n",
       " 'آلشخص',\n",
       " 'آلشخصيآت',\n",
       " 'آلشعور',\n",
       " 'آلشمس',\n",
       " 'آلشوق',\n",
       " 'آلشي',\n",
       " 'آلص',\n",
       " 'آلصبآح',\n",
       " 'آلصحبة',\n",
       " 'آلصحراوي',\n",
       " 'آلصدآقة',\n",
       " 'آلصدف',\n",
       " 'آلصصديق',\n",
       " 'آلصمت',\n",
       " 'آلصنم',\n",
       " 'آلض',\n",
       " 'آلضلـوع',\n",
       " 'آلضو',\n",
       " 'آلطـيب',\n",
       " 'آلطويل',\n",
       " 'آلطيب',\n",
       " 'آلطيبهہ',\n",
       " 'آلظروف',\n",
       " 'آلظن',\n",
       " 'آلع',\n",
       " 'آلعآفية',\n",
       " 'آلعآفيہ',\n",
       " 'آلعاالم',\n",
       " 'آلعبهآ',\n",
       " 'آلعدم',\n",
       " 'آلعرب',\n",
       " 'آلعسر',\n",
       " 'آلعشم',\n",
       " 'آلعطآء',\n",
       " 'آلعطر',\n",
       " 'آلعفو',\n",
       " 'آلعقل',\n",
       " 'آلعلآقهہ',\n",
       " 'آلعلاقه',\n",
       " 'آلعي',\n",
       " 'آلعيش',\n",
       " 'آلـ',\n",
       " 'آلـحم',\n",
       " 'آلـدنـيـا',\n",
       " 'آلـراحه',\n",
       " 'آلـرخ',\n",
       " 'آلـز',\n",
       " 'آلـصبآيـآ',\n",
       " 'آلـعفو',\n",
       " 'آلــذي',\n",
       " 'آلـناس',\n",
       " 'آلـگــل',\n",
       " 'آلف',\n",
       " 'آلفردوس',\n",
       " 'آلفـرح',\n",
       " 'آلق',\n",
       " 'آلقاق',\n",
       " 'آلقرآيب',\n",
       " 'آلقرايه',\n",
       " 'آلقلب',\n",
       " 'آلقلوب',\n",
       " 'آلقليل',\n",
       " 'آلقلۆب',\n",
       " 'آلقهر',\n",
       " 'آلكتآبہ',\n",
       " 'آلكذب',\n",
       " 'آلكريم',\n",
       " 'آلكل',\n",
       " 'آلل',\n",
       " 'آللآزم',\n",
       " 'آللآم',\n",
       " 'آللـي',\n",
       " 'آللقاء',\n",
       " 'آلله',\n",
       " 'آللهم',\n",
       " 'آللهہ',\n",
       " 'آللي',\n",
       " 'آللين',\n",
       " 'آللھ',\n",
       " 'آللھم',\n",
       " 'آللہ',\n",
       " 'آللہم',\n",
       " 'آلم',\n",
       " 'آلمآضي',\n",
       " 'آلمتك',\n",
       " 'آلمتلهفه',\n",
       " 'آلمجآمله',\n",
       " 'آلمجد',\n",
       " 'آلمح',\n",
       " 'آلمحبين',\n",
       " 'آلمس',\n",
       " 'آلمسآء',\n",
       " 'آلمستبيحة',\n",
       " 'آلمشتاق',\n",
       " 'آلمطلوب',\n",
       " 'آلمعيقلي',\n",
       " 'آلمعيوب',\n",
       " 'آلمقصود',\n",
       " 'آلمقـدره',\n",
       " 'آلمكم',\n",
       " 'آلمنا',\n",
       " 'آلمنهج',\n",
       " 'آلمه',\n",
       " 'آلمها',\n",
       " 'آلمهـونہ',\n",
       " 'آلمو',\n",
       " 'آلمي',\n",
       " 'آلميه',\n",
       " 'آلن',\n",
       " 'آلنآس',\n",
       " 'آلناس',\n",
       " 'آلنبي',\n",
       " 'آلنجم',\n",
       " 'آلنصيب',\n",
       " 'آلنظر',\n",
       " 'آلنــبضة',\n",
       " 'آلنـفس',\n",
       " 'آلنفس',\n",
       " 'آلنفـس',\n",
       " 'آلنقص',\n",
       " 'آلنهآر',\n",
       " 'آله',\n",
       " 'آلهة',\n",
       " 'آلهتكم',\n",
       " 'آلهزآيم',\n",
       " 'آلهموم',\n",
       " 'آلهه',\n",
       " 'آلهي',\n",
       " 'آلو',\n",
       " 'آلوحيد',\n",
       " 'آلورد',\n",
       " 'آلوصل',\n",
       " 'آلوفي',\n",
       " 'آلوكيل',\n",
       " 'آلى',\n",
       " 'آلي',\n",
       " 'آليا',\n",
       " 'آليات',\n",
       " 'آلية',\n",
       " 'آليك',\n",
       " 'آليكس',\n",
       " 'آليل',\n",
       " 'آلين',\n",
       " 'آلينآ',\n",
       " 'آليه',\n",
       " 'آليۆم',\n",
       " 'آلٱيـام',\n",
       " 'آلگون',\n",
       " 'آم',\n",
       " 'آمآ',\n",
       " 'آمآت',\n",
       " 'آما',\n",
       " 'آمال',\n",
       " 'آمالا',\n",
       " 'آمالك',\n",
       " 'آمالنا',\n",
       " 'آماله',\n",
       " 'آمالها',\n",
       " 'آمالى',\n",
       " 'آمالي',\n",
       " 'آمان',\n",
       " 'آمتصاص',\n",
       " 'آمج',\n",
       " 'آمد',\n",
       " 'آمر',\n",
       " 'آمرآة',\n",
       " 'آمراه',\n",
       " 'آمري',\n",
       " 'آمشي',\n",
       " 'آمطرت',\n",
       " 'آمعہ',\n",
       " 'آمــــين',\n",
       " 'آمــين',\n",
       " 'آمـيـن',\n",
       " 'آمـين',\n",
       " 'آمك',\n",
       " 'آمل',\n",
       " 'آملا',\n",
       " 'آملك',\n",
       " 'آملنا',\n",
       " 'آملها',\n",
       " 'آمن',\n",
       " 'آمنا',\n",
       " 'آمنة',\n",
       " 'آمنةوفي',\n",
       " 'آمنت',\n",
       " 'آمنتلهم',\n",
       " 'آمنه',\n",
       " 'آمنوا',\n",
       " 'آمنين',\n",
       " 'آمنيين',\n",
       " 'آموت',\n",
       " 'آمون',\n",
       " 'آمي',\n",
       " 'آميرھ',\n",
       " 'آميـــــــــــــــن',\n",
       " 'آمين',\n",
       " 'آمينا',\n",
       " 'آميين',\n",
       " 'آمييين',\n",
       " 'آميييين',\n",
       " 'آمييييين',\n",
       " 'آميييييين',\n",
       " 'آميييييييين',\n",
       " 'آميييييييييييييين',\n",
       " 'آمييييييييييييييييييييييييين',\n",
       " 'آمہم',\n",
       " 'آمۆآت',\n",
       " 'آمۆۆآح',\n",
       " 'آن',\n",
       " 'آنآ',\n",
       " 'آنآسا',\n",
       " 'آنئذ',\n",
       " 'آنا',\n",
       " 'آناقعدت',\n",
       " 'آنام',\n",
       " 'آناهد',\n",
       " 'آناوصلت',\n",
       " 'آنت',\n",
       " 'آنتظآرگ',\n",
       " 'آنتظار',\n",
       " 'آنتم',\n",
       " 'آنتهت',\n",
       " 'آنتوني',\n",
       " 'آنتي',\n",
       " 'آنتين',\n",
       " 'آندي',\n",
       " 'آنذاك',\n",
       " 'آنر',\n",
       " 'آنس',\n",
       " 'آنساتى',\n",
       " 'آنسة',\n",
       " 'آنستينا',\n",
       " 'آنسه',\n",
       " 'آنسها',\n",
       " 'آنع',\n",
       " 'آنغو',\n",
       " 'آنـا',\n",
       " 'آنـاا',\n",
       " 'آنـام',\n",
       " 'آنـت',\n",
       " 'آنـسـان',\n",
       " 'آنــا',\n",
       " 'آنف',\n",
       " 'آنفآسي',\n",
       " 'آنفا',\n",
       " 'آنفسنا',\n",
       " 'آنفولو',\n",
       " 'آنفيلد',\n",
       " 'آنقطــآع',\n",
       " 'آنك',\n",
       " 'آنكسار',\n",
       " 'آننآ',\n",
       " 'آنني',\n",
       " 'آنه',\n",
       " 'آنهآ',\n",
       " 'آنها',\n",
       " 'آنهارده',\n",
       " 'آنى',\n",
       " 'آني',\n",
       " 'آنيق',\n",
       " 'آنيقة',\n",
       " 'آنيمو',\n",
       " 'آنٺ',\n",
       " 'آنگ',\n",
       " 'آه',\n",
       " 'آهات',\n",
       " 'آهاتــــــي',\n",
       " 'آهاتك',\n",
       " 'آهاتي',\n",
       " 'آهتمإم',\n",
       " 'آهد',\n",
       " 'آهداف',\n",
       " 'آهر',\n",
       " 'آهـآ',\n",
       " 'آهــلا',\n",
       " 'آهـل',\n",
       " 'آهل',\n",
       " 'آهو',\n",
       " 'آهووو',\n",
       " 'آهوى',\n",
       " 'آهي',\n",
       " 'آو',\n",
       " 'آوالثيرآن',\n",
       " 'آوان',\n",
       " 'آوت',\n",
       " 'آود',\n",
       " 'آوصآلي',\n",
       " 'آوصيك',\n",
       " 'آوعدك',\n",
       " 'آوعى',\n",
       " 'آوقآت',\n",
       " 'آوليس',\n",
       " 'آووا',\n",
       " 'آوي',\n",
       " 'آى',\n",
       " 'آي',\n",
       " 'آيآت',\n",
       " 'آيآما',\n",
       " 'آيات',\n",
       " 'آياتنا',\n",
       " 'آياته',\n",
       " 'آيامك',\n",
       " 'آية',\n",
       " 'آيدينك',\n",
       " 'آيديه',\n",
       " 'آيس',\n",
       " 'آيسن',\n",
       " 'آيـات',\n",
       " 'آيــه',\n",
       " 'آيفون',\n",
       " 'آيلة',\n",
       " 'آيله',\n",
       " 'آيم',\n",
       " 'آينشتاين',\n",
       " 'آيه',\n",
       " 'آييو',\n",
       " 'آييوبس',\n",
       " 'آٺ',\n",
       " 'آڏآ',\n",
       " 'آڏن',\n",
       " 'آڒ',\n",
       " 'آڤاراول',\n",
       " 'آګث',\n",
       " 'آګم',\n",
       " 'آگ',\n",
       " 'آگتآفگم',\n",
       " 'آگتب',\n",
       " 'آگث',\n",
       " 'آگرمنا',\n",
       " 'آڳبر',\n",
       " 'آہ',\n",
       " 'آۆجآ',\n",
       " 'آۆط',\n",
       " 'آۆل',\n",
       " 'آۆڏي',\n",
       " 'آۆگ',\n",
       " 'آۆہ',\n",
       " 'آﭸٻڪ',\n",
       " 'آﻟجنة',\n",
       " 'آﻥ',\n",
       " 'أءذا',\n",
       " 'أءذى',\n",
       " 'أءنا',\n",
       " 'أآعيش',\n",
       " 'أأ',\n",
       " 'أأأأأاااه',\n",
       " 'أأأد',\n",
       " 'أأأه',\n",
       " 'أأجدع',\n",
       " 'أأجدعان',\n",
       " 'أأجل',\n",
       " 'أأحسد',\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix_df = pd.DataFrame(document_term_matrix_sps.toarray(),\n",
    "                                       index=df.index,\n",
    "                                       columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df.cleaned_text, document_term_matrix_df], axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute SVD of Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "SVD = TruncatedSVD(n_components)\n",
    "component_names = [\"component_\"+str(i+1) for i in range(n_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix = SVD.fit_transform(document_term_matrix_sps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix[:4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SVD Matrix with Documents and Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_semantic_analysis = pd.DataFrame(svd_matrix,\n",
    "                                        index=document_term_matrix_df.index,\n",
    "                                        columns=component_names)\n",
    "latent_semantic_analysis['cleaned_text'] = df.cleaned_text\n",
    "latent_semantic_analysis['class'] = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_semantic_analysis.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_loadings = pd.DataFrame(SVD.components_,\n",
    "                                   index=component_names,\n",
    "                                   columns=tfidf_vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_loadings['abs_component_1'] = np.abs(vocabulary_loadings.component_1)\n",
    "vocabulary_loadings['abs_component_2'] = np.abs(vocabulary_loadings.component_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA_Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Top Terms for Each Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Terms for Component 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_loadings.sort_values('abs_component_1',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Terms for Component 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_loadings.sort_values('abs_component_2',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Top Two Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "pc_1 = latent_semantic_analysis['component_1'].values\n",
    "pc_2 = latent_semantic_analysis['component_2'].values\n",
    "\n",
    "plt.scatter(pc_1, pc_2, c=df['class_numerical'], cmap='rainbow')\n",
    "\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.axvline(linewidth=0.5)\n",
    "plt.axhline(linewidth=0.5)\n",
    "plt.xlim(-.1,1)\n",
    "plt.ylim(-.5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "pc_1 = latent_semantic_analysis['component_1'].values\n",
    "pc_2 = latent_semantic_analysis['component_2'].values\n",
    "\n",
    "strings = df['cleaned_text'].values\n",
    "for i, (x, y) in enumerate(zip(pc_1, pc_2)): \n",
    "    plt.text(x,y,strings[i][:10])\n",
    "\n",
    "plt.scatter(pc_1, pc_2, c=df['class_numerical'], cmap='rainbow')\n",
    "\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.axvline(linewidth=0.5)\n",
    "plt.axhline(linewidth=0.5)\n",
    "plt.xlim(-.1,1)\n",
    "plt.ylim(-.1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "pc_1 = latent_semantic_analysis['component_1'].values\n",
    "pc_2 = latent_semantic_analysis['component_2'].values\n",
    "\n",
    "plt.scatter(pc_1, pc_2, c=df['class_numerical'], cmap='rainbow')\n",
    "\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.axvline(linewidth=0.5)\n",
    "plt.axhline(linewidth=0.5)\n",
    "plt.xlim(-.01,.5)\n",
    "plt.ylim(-.3,.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_mask = latent_semantic_analysis['class'] == 'EG'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_semantic_analysis[eg_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gulf_mask = latent_semantic_analysis['class'] == 'GULF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_semantic_analysis[gulf_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_semantic_analysis[(latent_semantic_analysis['class'] == 'EG') \n",
    "                         & (latent_semantic_analysis.component_2 > .050)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_semantic_analysis[(latent_semantic_analysis['class'] == 'GULF') \n",
    "                         & (latent_semantic_analysis.component_2 > .50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try `50` SVD components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 50\n",
    "SVD = TruncatedSVD(n_components)\n",
    "component_names = [\"component_\"+str(i+1) for i in range(n_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix = SVD.fit_transform(document_term_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(SVD.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_explained_variance_eg_gulf = np.cumsum(SVD.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_explained_variance_eg_gulf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the cumulative sum of the explained variance ratio from the `50` SVD components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "X = np.arange(1,51)\n",
    "cumulative_explained_variance_eg_gulf = np.cumsum(SVD.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(X, cumulative_explained_variance_eg_gulf, '-o')\n",
    "plt.bar(X, SVD.explained_variance_ratio_, align='center', alpha=0.5)\n",
    "\n",
    "for i, j in zip(X, np.cumsum(SVD.explained_variance_ratio_)):\n",
    "    plt.annotate(str(j.round(2)), xy=(i+.1,j-.01))\n",
    "    \n",
    "plt.xlabel('SVD components')\n",
    "plt.ylabel('Explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(50), np.cumsum(SVD.explained_variance_ratio_), label='cumulative explained variance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Top Terms for Each Component 'topics'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression = pd.DataFrame(SVD.components_,\n",
    "                                     index=component_names,\n",
    "                                     columns=tfidf_vectorizer.get_feature_names()).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1,11):\n",
    "    vocabulary_expression['abs_component_{}'.format(i)] = np.abs(vocabulary_expression['component_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_1'].sort_values(ascending=False).head(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_2'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_3'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_4'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_5'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_6'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_7'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_8'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_9'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_10'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the search term using the same vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sentences = [\n",
    "{\"sentence\": \"الثوره المصريه تحولت من ثورة شارع محدش يزعل\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"نفسي اكون زيك بعرف اطنشك أو اخليك اخر حاجة و بعد كده اضحك عليك بكلمتين و انت تصدق كل مرة عادي\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"بما أن أغلب اللي متابعني مش بقدر اوصلهم أغلب الوقت. . ف كل يوم هعمل تويته آخر اليوم اللي هيعمل لايك\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"مقاومتنا للأشياء طلعت بتقل مع الزمن، مبقيناش نناهد ف حاجة.. و مش عشان أحنا جامدين قوي. هو حيلنا بس\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"عارف ايه احلى حاجة حاصلة ليا انى منك وانت برضه بتجرى فيا انت اخر كل يوم باخدك ف حضنى وانت اول\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"القاضى اللى حكم على المعتقلين بالاعدام هو هو نفس القاضى اللى هيراقب الانتخابات\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"وفجأة تيجي سيرة حاجة في وسط الكلام تقلب عليك القديم والجديد وترسم في دماغك علامات استفهام مالهاش\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"السنة اللي فاتت الاعلام الانجليزي قال المفروض بيب يعرف انه في البريمييرليج لازم يتأقلم و يلعب كورتنا\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"حرب و قتال و ناس تموت و هذا الدلخ يقول سعيد و مثل أجواء كرة القدم \", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"من غباء الهلالي الدلخ اللي يفتخر بفوز فريقه من قيادة رئيس الحكام كلاتنبيرغ له سنه ماسنع الحكام السعوديين\", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"شفتوا هوشة شيعان وغالي لو هي بين الهلاليين كان شفتوا هاشتاق كبر راسهم المنسم وكان جاك هذا الدلخ \", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"ذويه ارفضوا لانه عيار جمبازي مافيه شي وبليس مايكسر اماعينه يامال لضعفه قطو بو سبعة ارواح \", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"صج ياجماعه في سوال محيرني ليش المتان مافيهم النفسيه عكس الضعاف تقول خاشوقه ومنفس\", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"لم نعاند التاريخ مسيو خاشوقه بل الواقع والعقلانية ابعدنا من التدمير والانفلات\", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"أي والله وعندي عنه ابو خاشوقة أسرار لا تشرف قد أقولها اذا لم يلجم لسانه عن سب وطني\", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"قبل ماتتكلمين يالطيبه افهمي السالفه ومنب ملزومه بسنابي اني اشرح كل شيء صارت بالتفصيل بس لانك قلق خل\", \"class\" : \"GULF\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sentences_df = pd.DataFrame(search_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sentences_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sentences_df.sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms_encoded = tfidf_vectorizer.transform(search_sentences_df.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms_encoded_df = pd.DataFrame(search_terms_encoded.toarray(), \n",
    "                                       index=search_sentences_df.sentence, \n",
    "                                       columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a Random Search Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_term_df = search_terms_encoded_df.sample(3)\n",
    "random_search_term_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the search term to the document term matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_with_search_term.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_with_search_term.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_with_search_term = document_term_matrix_df.append(random_search_term_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dtm_with_search_term.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_with_search_term.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df = pd.DataFrame(svd_matrix, \n",
    "                      index=dtm_with_search_term.index, \n",
    "                      columns=component_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_with_search_term[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Vector for our Search Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term_svd_vector = svd_df.loc[random_search_term_df.index]\n",
    "search_term_svd_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cosine Similarity to Find the Most Similar Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df['cosine_sim'] = cosine_similarity(svd_df, search_term_svd_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df[['cosine_sim']].sort_values('cosine_sim', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['class_numerical'], random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'يا جمهور سلطان الدررع والمراكز الاولى نبيها صوتو لسلطان رقم رصيدك'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [08:05<00:00, 30.37s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>max_df</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_val_score</th>\n",
       "      <th>min_df</th>\n",
       "      <th>ngram_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.763265</td>\n",
       "      <td>0.764495</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.763549</td>\n",
       "      <td>0.763507</td>\n",
       "      <td>4</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.762132</td>\n",
       "      <td>0.762602</td>\n",
       "      <td>4</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.761731</td>\n",
       "      <td>0.762450</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.760314</td>\n",
       "      <td>0.759907</td>\n",
       "      <td>8</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha  max_df  mean_train_score  mean_val_score  min_df ngram_range\n",
       "4   0.1    0.950   0.763265          0.764495        2       (1, 2)    \n",
       "8   0.1    0.950   0.763549          0.763507        4       (1, 2)    \n",
       "10  0.1    0.999   0.762132          0.762602        4       (1, 2)    \n",
       "6   0.1    0.999   0.761731          0.762450        2       (1, 2)    \n",
       "14  0.1    0.999   0.760314          0.759907        8       (1, 2)    "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [(ngr, mindf, maxdf, alpha)\n",
    "          for ngr in [(1,2)]\n",
    "          for mindf in [1,2,4,8]\n",
    "          for maxdf in np.linspace(.95,.999,2)\n",
    "          for alpha in np.logspace(-1,3,2)\n",
    "         ]\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for ngr, mindf, maxdf, alpha in tqdm(params):\n",
    "    results = {\n",
    "        'ngram_range' : ngr,\n",
    "        'min_df' : mindf,\n",
    "        'max_df' : maxdf,\n",
    "        'alpha' : alpha\n",
    "    }\n",
    "    train_scores = list()\n",
    "    val_scores = list()\n",
    "    \n",
    "    for train_indices, val_indices in skfold.split(X_train.astype('str'), y_train):\n",
    "        \n",
    "        X_train_kf, y_train_kf = X_train.iloc[train_indices], y_train.iloc[train_indices]\n",
    "        X_val_kf, y_val_kf = X_train.iloc[val_indices], y_train.iloc[val_indices]\n",
    "        \n",
    "        lsa_pipe = Pipeline([\n",
    "                                ('tfidf', TfidfVectorizer(ngram_range=ngr, min_df=mindf, max_df=maxdf)),\n",
    "                                ('svd', TruncatedSVD(50)),\n",
    "                                ('clf', RidgeClassifier(alpha=alpha))\n",
    "                            ])\n",
    "        \n",
    "        lsa_pipe.fit(X_train_kf, y_train_kf)\n",
    "        \n",
    "        train_scores.append(lsa_pipe.score(X_train_kf, y_train_kf))\n",
    "        val_scores.append(lsa_pipe.score(X_val_kf, y_val_kf))\n",
    "        \n",
    "    results['mean_train_score'] = np.mean(train_scores)\n",
    "    results['mean_val_score'] = np.mean(val_scores)\n",
    "    results_list.append(results)\n",
    "\n",
    "cv_results = pd.DataFrame(results_list)\n",
    "\n",
    "cv_results.sort_values('mean_val_score', ascending=False, inplace=True)\n",
    "\n",
    "alpha, maxdf, _, _, mindf, ngr = cv_results.iloc[0].values\n",
    "\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_pipe = Pipeline([\n",
    "                        ('tfidf', TfidfVectorizer(ngram_range=ngr, min_df=mindf, max_df=maxdf)),\n",
    "                        ('svd', TruncatedSVD(100)),\n",
    "                        ('clf', RidgeClassifier(alpha=alpha))\n",
    "                    ])\n",
    "\n",
    "history = lsa_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77973352130585616"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_pipe.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77893187378197948"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_pipe.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lsa_pipe.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.67      0.75     24328\n",
      "          1       0.72      0.89      0.80     23906\n",
      "\n",
      "avg / total       0.80      0.78      0.78     48234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.66      0.75     72924\n",
      "          1       0.72      0.91      0.80     71778\n",
      "\n",
      "avg / total       0.80      0.78      0.78    144702\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.67      0.75     24328\n",
      "          1       0.72      0.89      0.80     23906\n",
      "\n",
      "avg / total       0.80      0.78      0.78     48234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, lsa_pipe.predict(X_train)))\n",
    "print(classification_report(y_test, lsa_pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lsa_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16182  8146]\n",
      " [ 2517 21389]]\n"
     ]
    }
   ],
   "source": [
    "confmat = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-9478a7789ad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlsa_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "lsa_pipe.predict(X_train)[:, 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr.predict_proba(X_train)[:, 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6040483dc38c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsa_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'False Positive Rate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_train, lsa_pipe.predict(X_train)[:,1])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
