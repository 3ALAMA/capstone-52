{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark LSA & LDA \n",
    "\n",
    "___(Without stop words)___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access corpus through pickled MongoDB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/capstone-52/Pickled_from_mongo\n"
     ]
    }
   ],
   "source": [
    "cd ../../Pickled_from_mongo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_eg_gulf_1k_sample.p  combined_eg_gulf_200k_sample.p\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../Pickled_from_mongo/combined_eg_gulf_200k_sample.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192936, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['cleaned_geo','cleaned_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 192936 entries, 0 to 95683\n",
      "Data columns (total 3 columns):\n",
      "_id             192936 non-null object\n",
      "cleaned_text    192936 non-null object\n",
      "class           192936 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>cleaned_geo</th>\n",
       "      <th>cleaned_name</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60340</th>\n",
       "      <td>5a2cb974204c9e0400ceac96</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>ZeinJr_</td>\n",
       "      <td>كل سنه وإنتي طيبه يا قلبي</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>5a2d98404be10404f69a6194</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>white_pearla</td>\n",
       "      <td>الى هنا و ماقدررر أوله عليك أكثررر</td>\n",
       "      <td>GULF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46564</th>\n",
       "      <td>5a2ca043204c9e0400ce76c6</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Asemkhaledd</td>\n",
       "      <td>اللهم إنى وكلتك أمرى فكن لي خير وكيل، ودبر لي أمرى فإنى لا أُحسن التدبير</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _id cleaned_geo  cleaned_name  \\\n",
       "60340  5a2cb974204c9e0400ceac96  Egypt       ZeinJr_        \n",
       "9709   5a2d98404be10404f69a6194  Kuwait      white_pearla   \n",
       "46564  5a2ca043204c9e0400ce76c6  Egypt       Asemkhaledd    \n",
       "\n",
       "                                                                   cleaned_text  \\\n",
       "60340  كل سنه وإنتي طيبه يا قلبي                                                  \n",
       "9709   الى هنا و ماقدررر أوله عليك أكثررر                                         \n",
       "46564  اللهم إنى وكلتك أمرى فكن لي خير وكيل، ودبر لي أمرى فإنى لا أُحسن التدبير   \n",
       "\n",
       "      class  \n",
       "60340  EG    \n",
       "9709   GULF  \n",
       "46564  EG    "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark LSA with stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode the Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['class_numerical'] = le.fit_transform(df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "### Prepare Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix_sps = tfidf_vectorizer.fit_transform(df.cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<192936x223045 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1807963 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix_sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__',\n",
       " '¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼⅓¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼',\n",
       " 'éé',\n",
       " 'ąšşåň',\n",
       " 'ąℓï',\n",
       " 'ışı',\n",
       " 'ĸя',\n",
       " 'ńò',\n",
       " 'əŕ',\n",
       " 'ɹɹɹ',\n",
       " 'глаз',\n",
       " 'днем',\n",
       " 'из',\n",
       " 'моих',\n",
       " 'рождения',\n",
       " 'яαωαα',\n",
       " 'օℓℓօω',\n",
       " 'בــرام',\n",
       " 'בـوآلـي',\n",
       " 'בـﻤـﺪﻩ',\n",
       " 'וلي',\n",
       " 'ךצשךטל',\n",
       " 'םבםב',\n",
       " 'ءֆ',\n",
       " 'ءֆء',\n",
       " 'ءء',\n",
       " 'ءءءءءءءء',\n",
       " 'ءاخر',\n",
       " 'ءالتي',\n",
       " 'ءالله',\n",
       " 'ءدم',\n",
       " 'ءناس',\n",
       " 'ءوف',\n",
       " 'ءﺍﻣﻨﻮا',\n",
       " 'آء',\n",
       " 'آءة',\n",
       " 'آءت',\n",
       " 'آآ',\n",
       " 'آآء',\n",
       " 'آآآ',\n",
       " 'آآآآآح',\n",
       " 'آآآآح',\n",
       " 'آآآآمج',\n",
       " 'آآآآگ',\n",
       " 'آآآت',\n",
       " 'آآآج',\n",
       " 'آآآح',\n",
       " 'آآآسہ',\n",
       " 'آآآني',\n",
       " 'آآآه',\n",
       " 'آآآگي',\n",
       " 'آآج',\n",
       " 'آآح',\n",
       " 'آآس',\n",
       " 'آآسسہ',\n",
       " 'آآسعد',\n",
       " 'آآللي',\n",
       " 'آآمني',\n",
       " 'آآمين',\n",
       " 'آآمييين',\n",
       " 'آآه',\n",
       " 'آآهم',\n",
       " 'آآگ',\n",
       " 'آؤ',\n",
       " 'آؤل',\n",
       " 'آئ',\n",
       " 'آئئ',\n",
       " 'آئعہ',\n",
       " 'آئف',\n",
       " 'آئمآ',\n",
       " 'آئن',\n",
       " 'آئي',\n",
       " 'آاااااااااااااااه',\n",
       " 'آاااازوله',\n",
       " 'آاب',\n",
       " 'آات',\n",
       " 'آاح',\n",
       " 'آاحن',\n",
       " 'آاش',\n",
       " 'آب',\n",
       " 'آباءنا',\n",
       " 'آباءهم',\n",
       " 'آبائكم',\n",
       " 'آبائه',\n",
       " 'آبائهم',\n",
       " 'آبارا',\n",
       " 'آبت',\n",
       " 'آبتدى',\n",
       " 'آبتسآم',\n",
       " 'آبتسم',\n",
       " 'آبدا',\n",
       " 'آبذل',\n",
       " 'آبرن',\n",
       " 'آبسألك',\n",
       " 'آبعد',\n",
       " 'آبـرتـآإح',\n",
       " 'آبقى',\n",
       " 'آبل',\n",
       " 'آبه',\n",
       " 'آبواب',\n",
       " 'آبوس',\n",
       " 'آبوعدك',\n",
       " 'آبوووو',\n",
       " 'آبووووك',\n",
       " 'آبى',\n",
       " 'آبيض',\n",
       " 'آبيك',\n",
       " 'آة',\n",
       " 'آت',\n",
       " 'آتاا',\n",
       " 'آتاري',\n",
       " 'آتانا',\n",
       " 'آتاه',\n",
       " 'آتاها',\n",
       " 'آتاهم',\n",
       " 'آترك',\n",
       " 'آتركونا',\n",
       " 'آتسو',\n",
       " 'آتصبر',\n",
       " 'آتصل',\n",
       " 'آتغير',\n",
       " 'آتغيرت',\n",
       " 'آتــنا',\n",
       " 'آتكلمت',\n",
       " 'آتلفت',\n",
       " 'آتم',\n",
       " 'آتمسك',\n",
       " 'آتمق',\n",
       " 'آتمناه',\n",
       " 'آتمنى',\n",
       " 'آتنا',\n",
       " 'آتنازل',\n",
       " 'آتنفسها',\n",
       " 'آتنين',\n",
       " 'آتوب',\n",
       " 'آتى',\n",
       " 'آتي',\n",
       " 'آتية',\n",
       " 'آتيك',\n",
       " 'آتيه',\n",
       " 'آتگ',\n",
       " 'آث',\n",
       " 'آثاار',\n",
       " 'آثار',\n",
       " 'آثاره',\n",
       " 'آثارها',\n",
       " 'آثارهم',\n",
       " 'آثر',\n",
       " 'آثرتك',\n",
       " 'آثـ',\n",
       " 'آثق',\n",
       " 'آثما',\n",
       " 'آثمن',\n",
       " 'آثمه',\n",
       " 'آثنين',\n",
       " 'آج',\n",
       " 'آجبرك',\n",
       " 'آجتمع',\n",
       " 'آججم',\n",
       " 'آجدع',\n",
       " 'آجر',\n",
       " 'آجرب',\n",
       " 'آجرلهم',\n",
       " 'آجرن',\n",
       " 'آجرنا',\n",
       " 'آجرني',\n",
       " 'آجعل',\n",
       " 'آجعلك',\n",
       " 'آجعلني',\n",
       " 'آجعلهآ',\n",
       " 'آجــر',\n",
       " 'آجـل',\n",
       " 'آجـمل',\n",
       " 'آجل',\n",
       " 'آجلا',\n",
       " 'آجم',\n",
       " 'آجمعين',\n",
       " 'آجمل',\n",
       " 'آجنآس',\n",
       " 'آجه',\n",
       " 'آجى',\n",
       " 'آجي',\n",
       " 'آجيد',\n",
       " 'آح',\n",
       " 'آحب',\n",
       " 'آحبا',\n",
       " 'آحببتهم',\n",
       " 'آحببناها',\n",
       " 'آحبك',\n",
       " 'آحبكك',\n",
       " 'آحبه',\n",
       " 'آحت',\n",
       " 'آحتجتك',\n",
       " 'آحترآمك',\n",
       " 'آحترآمہ',\n",
       " 'آحترام',\n",
       " 'آحترامك',\n",
       " 'آحترم',\n",
       " 'آحتيآجي',\n",
       " 'آحد',\n",
       " 'آحرفي',\n",
       " 'آحس',\n",
       " 'آحسآس',\n",
       " 'آحسآسہ',\n",
       " 'آحساسي',\n",
       " 'آحساك',\n",
       " 'آحسنت',\n",
       " 'آحطك',\n",
       " 'آحـبــگ',\n",
       " 'آحل',\n",
       " 'آحلآم',\n",
       " 'آحلام',\n",
       " 'آحلى',\n",
       " 'آحمد',\n",
       " 'آحن',\n",
       " 'آحنا',\n",
       " 'آحوال',\n",
       " 'آحوالك',\n",
       " 'آحيآنآ',\n",
       " 'آحيآنا',\n",
       " 'آحيان',\n",
       " 'آحيانآ',\n",
       " 'آحيانا',\n",
       " 'آحيــــآنا',\n",
       " 'آخ',\n",
       " 'آخباري',\n",
       " 'آخبي',\n",
       " 'آختآرو',\n",
       " 'آختآروا',\n",
       " 'آخترت',\n",
       " 'آختكـ',\n",
       " 'آختيآره',\n",
       " 'آخد',\n",
       " 'آخدها',\n",
       " 'آخذ',\n",
       " 'آخذإجازه',\n",
       " 'آخذمصرف',\n",
       " 'آخذه',\n",
       " 'آخر',\n",
       " 'آخرا',\n",
       " 'آخرة',\n",
       " 'آخرتك',\n",
       " 'آخرته',\n",
       " 'آخرتها',\n",
       " 'آخرتي',\n",
       " 'آخرج',\n",
       " 'آخرك',\n",
       " 'آخره',\n",
       " 'آخرها',\n",
       " 'آخرهم',\n",
       " 'آخرون',\n",
       " 'آخرى',\n",
       " 'آخري',\n",
       " 'آخرين',\n",
       " 'آخسر',\n",
       " 'آخضر',\n",
       " 'آخـت',\n",
       " 'آخـر',\n",
       " 'آخـسر',\n",
       " 'آخـض',\n",
       " 'آخـــر',\n",
       " 'آخـف',\n",
       " 'آخـل',\n",
       " 'آخـڒ',\n",
       " 'آخلآقك',\n",
       " 'آخلق',\n",
       " 'آخن',\n",
       " 'آخود',\n",
       " 'آخوياا',\n",
       " 'آخيرا',\n",
       " 'آد',\n",
       " 'آداء',\n",
       " 'آداب',\n",
       " 'آدابه',\n",
       " 'آدبهم',\n",
       " 'آدة',\n",
       " 'آدد',\n",
       " 'آدرة',\n",
       " 'آدركت',\n",
       " 'آدركو',\n",
       " 'آدري',\n",
       " 'آدعواله',\n",
       " 'آدقه',\n",
       " 'آدم',\n",
       " 'آدمنته',\n",
       " 'آدمى',\n",
       " 'آدمي',\n",
       " 'آدمية',\n",
       " 'آدمين',\n",
       " 'آدميه',\n",
       " 'آدميين',\n",
       " 'آدو',\n",
       " 'آدى',\n",
       " 'آذآ',\n",
       " 'آذا',\n",
       " 'آذار',\n",
       " 'آذاك',\n",
       " 'آذاكم',\n",
       " 'آذان',\n",
       " 'آذاننا',\n",
       " 'آذانهم',\n",
       " 'آذاني',\n",
       " 'آذاه',\n",
       " 'آذتني',\n",
       " 'آذيتني',\n",
       " 'آذيته',\n",
       " 'آذيتوا',\n",
       " 'آذيه',\n",
       " 'آر',\n",
       " 'آرآ',\n",
       " 'آرآء',\n",
       " 'آرؤع',\n",
       " 'آراء',\n",
       " 'آراءنا',\n",
       " 'آراءهم',\n",
       " 'آراؤهم',\n",
       " 'آرائك',\n",
       " 'آرائنا',\n",
       " 'آرائه',\n",
       " 'آرائهم',\n",
       " 'آراك',\n",
       " 'آراها',\n",
       " 'آرب',\n",
       " 'آرت',\n",
       " 'آرتست',\n",
       " 'آرح',\n",
       " 'آرحم',\n",
       " 'آردوغانكم',\n",
       " 'آرزقني',\n",
       " 'آرس',\n",
       " 'آرسنال',\n",
       " 'آرض',\n",
       " 'آرضآء',\n",
       " 'آرضي',\n",
       " 'آركابي',\n",
       " 'آركــد',\n",
       " 'آرهآقه',\n",
       " 'آرو',\n",
       " 'آرواح',\n",
       " 'آروح',\n",
       " 'آروع',\n",
       " 'آرولار',\n",
       " 'آرى',\n",
       " 'آري',\n",
       " 'آريد',\n",
       " 'آرڒ',\n",
       " 'آرگ',\n",
       " 'آرہ',\n",
       " 'آرہب',\n",
       " 'آرہۆآمست',\n",
       " 'آز',\n",
       " 'آزار',\n",
       " 'آزاى',\n",
       " 'آزاي',\n",
       " 'آزر',\n",
       " 'آزعجني',\n",
       " 'آزوال',\n",
       " 'آزين',\n",
       " 'آس',\n",
       " 'آسأل',\n",
       " 'آسألك',\n",
       " 'آسئله',\n",
       " 'آسافر',\n",
       " 'آست',\n",
       " 'آسترخي',\n",
       " 'آستطيع',\n",
       " 'آستغفر',\n",
       " 'آستغفرالله',\n",
       " 'آستودعتگ',\n",
       " 'آستودعنآك',\n",
       " 'آسر',\n",
       " 'آسرقني',\n",
       " 'آسرني',\n",
       " 'آسعار',\n",
       " 'آسعد',\n",
       " 'آسعدتمـ',\n",
       " 'آسعدنا',\n",
       " 'آسـآلگ',\n",
       " 'آسـتر',\n",
       " 'آسـتغفرآللهہ',\n",
       " 'آسـمـگ',\n",
       " 'آسف',\n",
       " 'آسفة',\n",
       " 'آسفه',\n",
       " 'آسفين',\n",
       " 'آسقي',\n",
       " 'آسك',\n",
       " 'آسلوبك',\n",
       " 'آسم',\n",
       " 'آسمك',\n",
       " 'آسمي',\n",
       " 'آسهره',\n",
       " 'آسود',\n",
       " 'آسي',\n",
       " 'آسيا',\n",
       " 'آسيوي',\n",
       " 'آسيوية',\n",
       " 'آش',\n",
       " 'آشت',\n",
       " 'آشتآق',\n",
       " 'آشتهي',\n",
       " 'آشخاص',\n",
       " 'آشرب',\n",
       " 'آشع',\n",
       " 'آشعر',\n",
       " 'آشــتهي',\n",
       " 'آشك',\n",
       " 'آشكره',\n",
       " 'آشلي',\n",
       " 'آشهد',\n",
       " 'آشوآق',\n",
       " 'آشوفه',\n",
       " 'آشياء',\n",
       " 'آص',\n",
       " 'آصالح',\n",
       " 'آصبحت',\n",
       " 'آصبر',\n",
       " 'آصحى',\n",
       " 'آصدقاء',\n",
       " 'آصدقائهم',\n",
       " 'آصر',\n",
       " 'آصعب',\n",
       " 'آصلا',\n",
       " 'آض',\n",
       " 'آضــع',\n",
       " 'آضي',\n",
       " 'آط',\n",
       " 'آطآلة',\n",
       " 'آطري',\n",
       " 'آطيح',\n",
       " 'آع',\n",
       " 'آعافر',\n",
       " 'آعتذآر',\n",
       " 'آعجآب',\n",
       " 'آعد',\n",
       " 'آعدك',\n",
       " 'آعرف',\n",
       " 'آعشق',\n",
       " 'آعشقه',\n",
       " 'آعشقوآ',\n",
       " 'آعطآني',\n",
       " 'آعطت',\n",
       " 'آعطني',\n",
       " 'آعطى',\n",
       " 'آعطي',\n",
       " 'آعطيتہ',\n",
       " 'آعفو',\n",
       " 'آعلاميين',\n",
       " 'آعمآق',\n",
       " 'آعمق',\n",
       " 'آعي',\n",
       " 'آعيش',\n",
       " 'آعيشه',\n",
       " 'آغ',\n",
       " 'آغا',\n",
       " 'آغار',\n",
       " 'آغاى',\n",
       " 'آغت',\n",
       " 'آغدو',\n",
       " 'آغـا',\n",
       " 'آغـــلى',\n",
       " 'آغلقتـه',\n",
       " 'آغلى',\n",
       " 'آغيب',\n",
       " 'آف',\n",
       " 'آفاتارك',\n",
       " 'آفاق',\n",
       " 'آفاقها',\n",
       " 'آفة',\n",
       " 'آفت',\n",
       " 'آفتار',\n",
       " 'آفتارك',\n",
       " 'آفتح',\n",
       " 'آفتشك',\n",
       " 'آفضـح',\n",
       " 'آفعآلك',\n",
       " 'آفـآق',\n",
       " 'آفـــرد',\n",
       " 'آفـوت',\n",
       " 'آفكين',\n",
       " 'آفنوني',\n",
       " 'آفي',\n",
       " 'آفيق',\n",
       " 'آفييق',\n",
       " 'آفيہ',\n",
       " 'آق',\n",
       " 'آقبلونا',\n",
       " 'آقتنعت',\n",
       " 'آقداري',\n",
       " 'آقدس',\n",
       " 'آقسام',\n",
       " 'آقسو',\n",
       " 'آقفيت',\n",
       " 'آقول',\n",
       " 'آقولگ',\n",
       " 'آقين',\n",
       " 'آك',\n",
       " 'آكبر',\n",
       " 'آكبرمن',\n",
       " 'آكتب',\n",
       " 'آكتبك',\n",
       " 'آكتر',\n",
       " 'آكث',\n",
       " 'آكثر',\n",
       " 'آكرمني',\n",
       " 'آكشلي',\n",
       " 'آكشن',\n",
       " 'آكل',\n",
       " 'آكله',\n",
       " 'آكلوا',\n",
       " 'آكون',\n",
       " 'آكي',\n",
       " 'آكيد',\n",
       " 'آل',\n",
       " 'آلآ',\n",
       " 'آلآة',\n",
       " 'آلآح',\n",
       " 'آلآحلآم',\n",
       " 'آلآختنآق',\n",
       " 'آلآختيآر',\n",
       " 'آلآخرة',\n",
       " 'آلآخـرين',\n",
       " 'آلآخيآر',\n",
       " 'آلآر',\n",
       " 'آلآشخآص',\n",
       " 'آلآعلى',\n",
       " 'آلآف',\n",
       " 'آلآن',\n",
       " 'آلآنتقام',\n",
       " 'آلآهتمآم',\n",
       " 'آلآوآدم',\n",
       " 'آلآورآق',\n",
       " 'آلأبيض',\n",
       " 'آلأحسآس',\n",
       " 'آلأخ',\n",
       " 'آلأخرون',\n",
       " 'آلأدب',\n",
       " 'آلأصنآم',\n",
       " 'آلألوآن',\n",
       " 'آلأولـــى',\n",
       " 'آلإحترآم',\n",
       " 'آلا',\n",
       " 'آلاء',\n",
       " 'آلات',\n",
       " 'آلاخلاق',\n",
       " 'آلاشو',\n",
       " 'آلاشياء',\n",
       " 'آلاعلام',\n",
       " 'آلاف',\n",
       " 'آلافات',\n",
       " 'آلاقي',\n",
       " 'آلام',\n",
       " 'آلامثل',\n",
       " 'آلامك',\n",
       " 'آلامل',\n",
       " 'آلامنا',\n",
       " 'آلامه',\n",
       " 'آلامها',\n",
       " 'آلامي',\n",
       " 'آلاميال',\n",
       " 'آلان',\n",
       " 'آلاوقآت',\n",
       " 'آلب',\n",
       " 'آلبآل',\n",
       " 'آلبسيطہ',\n",
       " 'آلبشر',\n",
       " 'آلبشريه',\n",
       " 'آلبشـ',\n",
       " 'آلبعض',\n",
       " 'آلبنات',\n",
       " 'آلبنت',\n",
       " 'آلبڪآء',\n",
       " 'آلة',\n",
       " 'آلت',\n",
       " 'آلترآب',\n",
       " 'آلتسآمح',\n",
       " 'آلتسامح',\n",
       " 'آلتصرفآت',\n",
       " 'آلتفآؤ',\n",
       " 'آلتمنى',\n",
       " 'آلته',\n",
       " 'آلتي',\n",
       " 'آلث',\n",
       " 'آلثقل',\n",
       " 'آلج',\n",
       " 'آلجرح',\n",
       " 'آلجـمآيل',\n",
       " 'آلجم',\n",
       " 'آلجميلہ',\n",
       " 'آلجن',\n",
       " 'آلجنه',\n",
       " 'آلجيـل',\n",
       " 'آلح',\n",
       " 'آلحآل',\n",
       " 'آلحب',\n",
       " 'آلحرام',\n",
       " 'آلحزن',\n",
       " 'آلحزين',\n",
       " 'آلحـآل',\n",
       " 'آلحــيـــآة',\n",
       " 'آلحقيقية',\n",
       " 'آلحلوين',\n",
       " 'آلحلوہ',\n",
       " 'آلحمد',\n",
       " 'آلحمدلله',\n",
       " 'آلحنين',\n",
       " 'آلحيآة',\n",
       " 'آلحيآه',\n",
       " 'آلحياة',\n",
       " 'آلحيين',\n",
       " 'آلخ',\n",
       " 'آلخآتمة',\n",
       " 'آلخآتمہ',\n",
       " 'آلختام',\n",
       " 'آلخشره',\n",
       " 'آلخـآطـر',\n",
       " 'آلخلق',\n",
       " 'آلخي',\n",
       " 'آلخيبآت',\n",
       " 'آلخير',\n",
       " 'آلخيـر',\n",
       " 'آلد',\n",
       " 'آلدم',\n",
       " 'آلدنيآ',\n",
       " 'آلدنيا',\n",
       " 'آلدهر',\n",
       " 'آلذآت',\n",
       " 'آلذكرى',\n",
       " 'آلذنوب',\n",
       " 'آلذي',\n",
       " 'آلر',\n",
       " 'آلرت',\n",
       " 'آلرجآء',\n",
       " 'آلرجاء',\n",
       " 'آلرزق',\n",
       " 'آلزين',\n",
       " 'آلس',\n",
       " 'آلسرير',\n",
       " 'آلسعآده',\n",
       " 'آلسعاده',\n",
       " 'آلسعود',\n",
       " 'آلسـعآده',\n",
       " 'آلسمآء',\n",
       " 'آلسماء',\n",
       " 'آلش',\n",
       " 'آلشتآء',\n",
       " 'آلشتويه',\n",
       " 'آلشخص',\n",
       " 'آلشخصيآت',\n",
       " 'آلشعور',\n",
       " 'آلشمس',\n",
       " 'آلشوق',\n",
       " 'آلشي',\n",
       " 'آلص',\n",
       " 'آلصبآح',\n",
       " 'آلصحبة',\n",
       " 'آلصحراوي',\n",
       " 'آلصدآقة',\n",
       " 'آلصدف',\n",
       " 'آلصصديق',\n",
       " 'آلصمت',\n",
       " 'آلصنم',\n",
       " 'آلض',\n",
       " 'آلضلـوع',\n",
       " 'آلضو',\n",
       " 'آلطـيب',\n",
       " 'آلطويل',\n",
       " 'آلطيب',\n",
       " 'آلطيبهہ',\n",
       " 'آلظروف',\n",
       " 'آلظن',\n",
       " 'آلع',\n",
       " 'آلعآفية',\n",
       " 'آلعآفيہ',\n",
       " 'آلعاالم',\n",
       " 'آلعبهآ',\n",
       " 'آلعدم',\n",
       " 'آلعرب',\n",
       " 'آلعسر',\n",
       " 'آلعشم',\n",
       " 'آلعطآء',\n",
       " 'آلعطر',\n",
       " 'آلعفو',\n",
       " 'آلعقل',\n",
       " 'آلعلآقهہ',\n",
       " 'آلعلاقه',\n",
       " 'آلعي',\n",
       " 'آلعيش',\n",
       " 'آلـ',\n",
       " 'آلـحم',\n",
       " 'آلـدنـيـا',\n",
       " 'آلـراحه',\n",
       " 'آلـرخ',\n",
       " 'آلـز',\n",
       " 'آلـصبآيـآ',\n",
       " 'آلـعفو',\n",
       " 'آلــذي',\n",
       " 'آلـناس',\n",
       " 'آلـگــل',\n",
       " 'آلف',\n",
       " 'آلفردوس',\n",
       " 'آلفـرح',\n",
       " 'آلق',\n",
       " 'آلقاق',\n",
       " 'آلقرآيب',\n",
       " 'آلقرايه',\n",
       " 'آلقلب',\n",
       " 'آلقلوب',\n",
       " 'آلقليل',\n",
       " 'آلقلۆب',\n",
       " 'آلقهر',\n",
       " 'آلكتآبہ',\n",
       " 'آلكذب',\n",
       " 'آلكريم',\n",
       " 'آلكل',\n",
       " 'آلل',\n",
       " 'آللآزم',\n",
       " 'آللآم',\n",
       " 'آللـي',\n",
       " 'آللقاء',\n",
       " 'آلله',\n",
       " 'آللهم',\n",
       " 'آللهہ',\n",
       " 'آللي',\n",
       " 'آللين',\n",
       " 'آللھ',\n",
       " 'آللھم',\n",
       " 'آللہ',\n",
       " 'آللہم',\n",
       " 'آلم',\n",
       " 'آلمآضي',\n",
       " 'آلمتك',\n",
       " 'آلمتلهفه',\n",
       " 'آلمجآمله',\n",
       " 'آلمجد',\n",
       " 'آلمح',\n",
       " 'آلمحبين',\n",
       " 'آلمس',\n",
       " 'آلمسآء',\n",
       " 'آلمستبيحة',\n",
       " 'آلمشتاق',\n",
       " 'آلمطلوب',\n",
       " 'آلمعيقلي',\n",
       " 'آلمعيوب',\n",
       " 'آلمقصود',\n",
       " 'آلمقـدره',\n",
       " 'آلمكم',\n",
       " 'آلمنا',\n",
       " 'آلمنهج',\n",
       " 'آلمه',\n",
       " 'آلمها',\n",
       " 'آلمهـونہ',\n",
       " 'آلمو',\n",
       " 'آلمي',\n",
       " 'آلميه',\n",
       " 'آلن',\n",
       " 'آلنآس',\n",
       " 'آلناس',\n",
       " 'آلنبي',\n",
       " 'آلنجم',\n",
       " 'آلنصيب',\n",
       " 'آلنظر',\n",
       " 'آلنــبضة',\n",
       " 'آلنـفس',\n",
       " 'آلنفس',\n",
       " 'آلنفـس',\n",
       " 'آلنقص',\n",
       " 'آلنهآر',\n",
       " 'آله',\n",
       " 'آلهة',\n",
       " 'آلهتكم',\n",
       " 'آلهزآيم',\n",
       " 'آلهموم',\n",
       " 'آلهه',\n",
       " 'آلهي',\n",
       " 'آلو',\n",
       " 'آلوحيد',\n",
       " 'آلورد',\n",
       " 'آلوصل',\n",
       " 'آلوفي',\n",
       " 'آلوكيل',\n",
       " 'آلى',\n",
       " 'آلي',\n",
       " 'آليا',\n",
       " 'آليات',\n",
       " 'آلية',\n",
       " 'آليك',\n",
       " 'آليكس',\n",
       " 'آليل',\n",
       " 'آلين',\n",
       " 'آلينآ',\n",
       " 'آليه',\n",
       " 'آليۆم',\n",
       " 'آلٱيـام',\n",
       " 'آلگون',\n",
       " 'آم',\n",
       " 'آمآ',\n",
       " 'آمآت',\n",
       " 'آما',\n",
       " 'آمال',\n",
       " 'آمالا',\n",
       " 'آمالك',\n",
       " 'آمالنا',\n",
       " 'آماله',\n",
       " 'آمالها',\n",
       " 'آمالى',\n",
       " 'آمالي',\n",
       " 'آمان',\n",
       " 'آمتصاص',\n",
       " 'آمج',\n",
       " 'آمد',\n",
       " 'آمر',\n",
       " 'آمرآة',\n",
       " 'آمراه',\n",
       " 'آمري',\n",
       " 'آمشي',\n",
       " 'آمطرت',\n",
       " 'آمعہ',\n",
       " 'آمــــين',\n",
       " 'آمــين',\n",
       " 'آمـيـن',\n",
       " 'آمـين',\n",
       " 'آمك',\n",
       " 'آمل',\n",
       " 'آملا',\n",
       " 'آملك',\n",
       " 'آملنا',\n",
       " 'آملها',\n",
       " 'آمن',\n",
       " 'آمنا',\n",
       " 'آمنة',\n",
       " 'آمنةوفي',\n",
       " 'آمنت',\n",
       " 'آمنتلهم',\n",
       " 'آمنه',\n",
       " 'آمنوا',\n",
       " 'آمنين',\n",
       " 'آمنيين',\n",
       " 'آموت',\n",
       " 'آمون',\n",
       " 'آمي',\n",
       " 'آميرھ',\n",
       " 'آميـــــــــــــــن',\n",
       " 'آمين',\n",
       " 'آمينا',\n",
       " 'آميين',\n",
       " 'آمييين',\n",
       " 'آميييين',\n",
       " 'آمييييين',\n",
       " 'آميييييين',\n",
       " 'آميييييييين',\n",
       " 'آميييييييييييييين',\n",
       " 'آمييييييييييييييييييييييييين',\n",
       " 'آمہم',\n",
       " 'آمۆآت',\n",
       " 'آمۆۆآح',\n",
       " 'آن',\n",
       " 'آنآ',\n",
       " 'آنآسا',\n",
       " 'آنئذ',\n",
       " 'آنا',\n",
       " 'آناقعدت',\n",
       " 'آنام',\n",
       " 'آناهد',\n",
       " 'آناوصلت',\n",
       " 'آنت',\n",
       " 'آنتظآرگ',\n",
       " 'آنتظار',\n",
       " 'آنتم',\n",
       " 'آنتهت',\n",
       " 'آنتوني',\n",
       " 'آنتي',\n",
       " 'آنتين',\n",
       " 'آندي',\n",
       " 'آنذاك',\n",
       " 'آنر',\n",
       " 'آنس',\n",
       " 'آنساتى',\n",
       " 'آنسة',\n",
       " 'آنستينا',\n",
       " 'آنسه',\n",
       " 'آنسها',\n",
       " 'آنع',\n",
       " 'آنغو',\n",
       " 'آنـا',\n",
       " 'آنـاا',\n",
       " 'آنـام',\n",
       " 'آنـت',\n",
       " 'آنـسـان',\n",
       " 'آنــا',\n",
       " 'آنف',\n",
       " 'آنفآسي',\n",
       " 'آنفا',\n",
       " 'آنفسنا',\n",
       " 'آنفولو',\n",
       " 'آنفيلد',\n",
       " 'آنقطــآع',\n",
       " 'آنك',\n",
       " 'آنكسار',\n",
       " 'آننآ',\n",
       " 'آنني',\n",
       " 'آنه',\n",
       " 'آنهآ',\n",
       " 'آنها',\n",
       " 'آنهارده',\n",
       " 'آنى',\n",
       " 'آني',\n",
       " 'آنيق',\n",
       " 'آنيقة',\n",
       " 'آنيمو',\n",
       " 'آنٺ',\n",
       " 'آنگ',\n",
       " 'آه',\n",
       " 'آهات',\n",
       " 'آهاتــــــي',\n",
       " 'آهاتك',\n",
       " 'آهاتي',\n",
       " 'آهتمإم',\n",
       " 'آهد',\n",
       " 'آهداف',\n",
       " 'آهر',\n",
       " 'آهـآ',\n",
       " 'آهــلا',\n",
       " 'آهـل',\n",
       " 'آهل',\n",
       " 'آهو',\n",
       " 'آهووو',\n",
       " 'آهوى',\n",
       " 'آهي',\n",
       " 'آو',\n",
       " 'آوالثيرآن',\n",
       " 'آوان',\n",
       " 'آوت',\n",
       " 'آود',\n",
       " 'آوصآلي',\n",
       " 'آوصيك',\n",
       " 'آوعدك',\n",
       " 'آوعى',\n",
       " 'آوقآت',\n",
       " 'آوليس',\n",
       " 'آووا',\n",
       " 'آوي',\n",
       " 'آى',\n",
       " 'آي',\n",
       " 'آيآت',\n",
       " 'آيآما',\n",
       " 'آيات',\n",
       " 'آياتنا',\n",
       " 'آياته',\n",
       " 'آيامك',\n",
       " 'آية',\n",
       " 'آيدينك',\n",
       " 'آيديه',\n",
       " 'آيس',\n",
       " 'آيسن',\n",
       " 'آيـات',\n",
       " 'آيــه',\n",
       " 'آيفون',\n",
       " 'آيلة',\n",
       " 'آيله',\n",
       " 'آيم',\n",
       " 'آينشتاين',\n",
       " 'آيه',\n",
       " 'آييو',\n",
       " 'آييوبس',\n",
       " 'آٺ',\n",
       " 'آڏآ',\n",
       " 'آڏن',\n",
       " 'آڒ',\n",
       " 'آڤاراول',\n",
       " 'آګث',\n",
       " 'آګم',\n",
       " 'آگ',\n",
       " 'آگتآفگم',\n",
       " 'آگتب',\n",
       " 'آگث',\n",
       " 'آگرمنا',\n",
       " 'آڳبر',\n",
       " 'آہ',\n",
       " 'آۆجآ',\n",
       " 'آۆط',\n",
       " 'آۆل',\n",
       " 'آۆڏي',\n",
       " 'آۆگ',\n",
       " 'آۆہ',\n",
       " 'آﭸٻڪ',\n",
       " 'آﻟجنة',\n",
       " 'آﻥ',\n",
       " 'أءذا',\n",
       " 'أءذى',\n",
       " 'أءنا',\n",
       " 'أآعيش',\n",
       " 'أأ',\n",
       " 'أأأأأاااه',\n",
       " 'أأأد',\n",
       " 'أأأه',\n",
       " 'أأجدع',\n",
       " 'أأجدعان',\n",
       " 'أأجل',\n",
       " 'أأحسد',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-90a19c412cfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m document_term_matrix_df = pd.DataFrame(document_term_matrix_sps.toarray(),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                        \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                        columns=tfidf_vectorizer.get_feature_names())\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;31m##############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "document_term_matrix_df = pd.DataFrame(document_term_matrix_sps.toarray(),\n",
    "                                       index=df.index,\n",
    "                                       columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df.cleaned_text, document_term_matrix_df], axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute SVD of Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "SVD = TruncatedSVD(n_components)\n",
    "component_names = [\"component_\"+str(i+1) for i in range(n_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix = SVD.fit_transform(document_term_matrix_sps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix[:4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SVD Matrix with Documents and Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_semantic_analysis = pd.DataFrame(svd_matrix,\n",
    "                                        index=df.index,\n",
    "                                        columns=component_names)\n",
    "latent_semantic_analysis['cleaned_text'] = df.cleaned_text\n",
    "latent_semantic_analysis['class'] = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_semantic_analysis.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_loadings = pd.DataFrame(SVD.components_,\n",
    "                                   index=component_names,\n",
    "                                   columns=tfidf_vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_loadings['abs_component_1'] = np.abs(vocabulary_loadings.component_1)\n",
    "vocabulary_loadings['abs_component_2'] = np.abs(vocabulary_loadings.component_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA_Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Top Terms for Each Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Terms for Component 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_loadings.sort_values('abs_component_1',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Terms for Component 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_loadings.sort_values('abs_component_2',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Top Two Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "pc_1 = latent_semantic_analysis['component_1'].values\n",
    "pc_2 = latent_semantic_analysis['component_2'].values\n",
    "\n",
    "plt.scatter(pc_1, pc_2, c=df['class_numerical'], cmap='rainbow')\n",
    "\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.axvline(linewidth=0.5)\n",
    "plt.axhline(linewidth=0.5)\n",
    "plt.xlim(-.1,1)\n",
    "plt.ylim(-.5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "pc_1 = latent_semantic_analysis['component_1'].values\n",
    "pc_2 = latent_semantic_analysis['component_2'].values\n",
    "\n",
    "strings = df['cleaned_text'].values\n",
    "for i, (x, y) in enumerate(zip(pc_1, pc_2)): \n",
    "    plt.text(x,y,strings[i][:10])\n",
    "\n",
    "plt.scatter(pc_1, pc_2, c=df['class_numerical'], cmap='rainbow')\n",
    "\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.axvline(linewidth=0.5)\n",
    "plt.axhline(linewidth=0.5)\n",
    "plt.xlim(-.1,1)\n",
    "plt.ylim(-.1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "pc_1 = latent_semantic_analysis['component_1'].values\n",
    "pc_2 = latent_semantic_analysis['component_2'].values\n",
    "\n",
    "plt.scatter(pc_1, pc_2, c=df['class_numerical'], cmap='rainbow')\n",
    "\n",
    "plt.xlabel('First PC')\n",
    "plt.ylabel('Second PC')\n",
    "plt.axvline(linewidth=0.5)\n",
    "plt.axhline(linewidth=0.5)\n",
    "plt.xlim(-.01,.5)\n",
    "plt.ylim(-.3,.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_mask = latent_semantic_analysis['class'] == 'EG'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_semantic_analysis[eg_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gulf_mask = latent_semantic_analysis['class'] == 'GULF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_semantic_analysis[gulf_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_semantic_analysis[(latent_semantic_analysis['class'] == 'EG') \n",
    "                         & (latent_semantic_analysis.component_2 > .050)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_semantic_analysis[(latent_semantic_analysis['class'] == 'GULF') \n",
    "                         & (latent_semantic_analysis.component_2 > .50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try `50` SVD components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 50\n",
    "SVD = TruncatedSVD(n_components)\n",
    "component_names = [\"component_\"+str(i+1) for i in range(n_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix = SVD.fit_transform(document_term_matrix_sps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(SVD.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_explained_variance_eg_gulf = np.cumsum(SVD.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_explained_variance_eg_gulf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the cumulative sum of the explained variance ratio from the `50` SVD components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "X = np.arange(1,51)\n",
    "cumulative_explained_variance_eg_gulf = np.cumsum(SVD.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(X, cumulative_explained_variance_eg_gulf, '-o')\n",
    "plt.bar(X, SVD.explained_variance_ratio_, align='center', alpha=0.5)\n",
    "\n",
    "for i, j in zip(X, np.cumsum(SVD.explained_variance_ratio_)):\n",
    "    plt.annotate(str(j.round(2)), xy=(i+.1,j-.01))\n",
    "    \n",
    "plt.xlabel('SVD components')\n",
    "plt.ylabel('Explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(50), np.cumsum(SVD.explained_variance_ratio_), label='cumulative explained variance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Top Terms for Each Component 'topics'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression = pd.DataFrame(SVD.components_,\n",
    "                                     index=component_names,\n",
    "                                     columns=tfidf_vectorizer.get_feature_names()).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1,11):\n",
    "    vocabulary_expression['abs_component_{}'.format(i)] = np.abs(vocabulary_expression['component_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_1'].sort_values(ascending=False).head(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_2'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_3'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_4'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_5'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_6'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_7'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_8'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_9'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_10'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the search terms using the same vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sentences = [\n",
    "{\"sentence\": \"الثوره المصريه تحولت من ثورة شارع محدش يزعل\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"نفسي اكون زيك بعرف اطنشك أو اخليك اخر حاجة و بعد كده اضحك عليك بكلمتين و انت تصدق كل مرة عادي\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"بما أن أغلب اللي متابعني مش بقدر اوصلهم أغلب الوقت. . ف كل يوم هعمل تويته آخر اليوم اللي هيعمل لايك\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"مقاومتنا للأشياء طلعت بتقل مع الزمن، مبقيناش نناهد ف حاجة.. و مش عشان أحنا جامدين قوي. هو حيلنا بس\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"عارف ايه احلى حاجة حاصلة ليا انى منك وانت برضه بتجرى فيا انت اخر كل يوم باخدك ف حضنى وانت اول\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"القاضى اللى حكم على المعتقلين بالاعدام هو هو نفس القاضى اللى هيراقب الانتخابات\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"وفجأة تيجي سيرة حاجة في وسط الكلام تقلب عليك القديم والجديد وترسم في دماغك علامات استفهام مالهاش\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"السنة اللي فاتت الاعلام الانجليزي قال المفروض بيب يعرف انه في البريمييرليج لازم يتأقلم و يلعب كورتنا\", \"class\" : \"EG\"},\n",
    "{\"sentence\": \"حرب و قتال و ناس تموت و هذا الدلخ يقول سعيد و مثل أجواء كرة القدم \", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"من غباء الهلالي الدلخ اللي يفتخر بفوز فريقه من قيادة رئيس الحكام كلاتنبيرغ له سنه ماسنع الحكام السعوديين\", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"شفتوا هوشة شيعان وغالي لو هي بين الهلاليين كان شفتوا هاشتاق كبر راسهم المنسم وكان جاك هذا الدلخ \", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"ذويه ارفضوا لانه عيار جمبازي مافيه شي وبليس مايكسر اماعينه يامال لضعفه قطو بو سبعة ارواح \", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"صج ياجماعه في سوال محيرني ليش المتان مافيهم النفسيه عكس الضعاف تقول خاشوقه ومنفس\", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"لم نعاند التاريخ مسيو خاشوقه بل الواقع والعقلانية ابعدنا من التدمير والانفلات\", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"أي والله وعندي عنه ابو خاشوقة أسرار لا تشرف قد أقولها اذا لم يلجم لسانه عن سب وطني\", \"class\" : \"GULF\"},\n",
    "{\"sentence\": \"قبل ماتتكلمين يالطيبه افهمي السالفه ومنب ملزومه بسنابي اني اشرح كل شيء صارت بالتفصيل بس لانك قلق خل\", \"class\" : \"GULF\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sentences_df = pd.DataFrame(search_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GULF</td>\n",
       "      <td>ذويه ارفضوا لانه عيار جمبازي مافيه شي وبليس مايكسر اماعينه يامال لضعفه قطو بو سبعة ارواح</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GULF</td>\n",
       "      <td>شفتوا هوشة شيعان وغالي لو هي بين الهلاليين كان شفتوا هاشتاق كبر راسهم المنسم وكان جاك هذا الدلخ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EG</td>\n",
       "      <td>الثوره المصريه تحولت من ثورة شارع محدش يزعل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GULF</td>\n",
       "      <td>من غباء الهلالي الدلخ اللي يفتخر بفوز فريقه من قيادة رئيس الحكام كلاتنبيرغ له سنه ماسنع الحكام السعوديين</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  \\\n",
       "11  GULF   \n",
       "10  GULF   \n",
       "0   EG     \n",
       "9   GULF   \n",
       "\n",
       "                                                                                                    sentence  \n",
       "11  ذويه ارفضوا لانه عيار جمبازي مافيه شي وبليس مايكسر اماعينه يامال لضعفه قطو بو سبعة ارواح                  \n",
       "10  شفتوا هوشة شيعان وغالي لو هي بين الهلاليين كان شفتوا هاشتاق كبر راسهم المنسم وكان جاك هذا الدلخ           \n",
       "0   الثوره المصريه تحولت من ثورة شارع محدش يزعل                                                               \n",
       "9   من غباء الهلالي الدلخ اللي يفتخر بفوز فريقه من قيادة رئيس الحكام كلاتنبيرغ له سنه ماسنع الحكام السعوديين  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_sentences_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     الثوره المصريه تحولت من ثورة شارع محدش يزعل                                                             \n",
       "1     نفسي اكون زيك بعرف اطنشك أو اخليك اخر حاجة و بعد كده اضحك عليك بكلمتين و انت تصدق كل مرة عادي           \n",
       "2     بما أن أغلب اللي متابعني مش بقدر اوصلهم أغلب الوقت. . ف كل يوم هعمل تويته آخر اليوم اللي هيعمل لايك     \n",
       "3     مقاومتنا للأشياء طلعت بتقل مع الزمن، مبقيناش نناهد ف حاجة.. و مش عشان أحنا جامدين قوي. هو حيلنا بس      \n",
       "4     عارف ايه احلى حاجة حاصلة ليا انى منك وانت برضه بتجرى فيا انت اخر كل يوم باخدك ف حضنى وانت اول           \n",
       "5     القاضى اللى حكم على المعتقلين بالاعدام هو هو نفس القاضى اللى هيراقب الانتخابات                          \n",
       "6     وفجأة تيجي سيرة حاجة في وسط الكلام تقلب عليك القديم والجديد وترسم في دماغك علامات استفهام مالهاش        \n",
       "7     السنة اللي فاتت الاعلام الانجليزي قال المفروض بيب يعرف انه في البريمييرليج لازم يتأقلم و يلعب كورتنا    \n",
       "8     حرب و قتال و ناس تموت و هذا الدلخ يقول سعيد و مثل أجواء كرة القدم                                       \n",
       "9     من غباء الهلالي الدلخ اللي يفتخر بفوز فريقه من قيادة رئيس الحكام كلاتنبيرغ له سنه ماسنع الحكام السعوديين\n",
       "10    شفتوا هوشة شيعان وغالي لو هي بين الهلاليين كان شفتوا هاشتاق كبر راسهم المنسم وكان جاك هذا الدلخ         \n",
       "11    ذويه ارفضوا لانه عيار جمبازي مافيه شي وبليس مايكسر اماعينه يامال لضعفه قطو بو سبعة ارواح                \n",
       "12    صج ياجماعه في سوال محيرني ليش المتان مافيهم النفسيه عكس الضعاف تقول خاشوقه ومنفس                        \n",
       "13    لم نعاند التاريخ مسيو خاشوقه بل الواقع والعقلانية ابعدنا من التدمير والانفلات                           \n",
       "14    أي والله وعندي عنه ابو خاشوقة أسرار لا تشرف قد أقولها اذا لم يلجم لسانه عن سب وطني                      \n",
       "15    قبل ماتتكلمين يالطيبه افهمي السالفه ومنب ملزومه بسنابي اني اشرح كل شيء صارت بالتفصيل بس لانك قلق خل     \n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_sentences_df.sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-23248b61afa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch_terms_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_sentences_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "search_terms_encoded = tfidf_vectorizer.transform(search_sentences_df.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms_encoded_df = pd.DataFrame(search_terms_encoded.toarray(), \n",
    "                                       index=search_sentences_df.sentence, \n",
    "                                       columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a Random Search Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_term_df = search_terms_encoded_df.sample(3)\n",
    "random_search_term_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the search term to the document term matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_with_search_term.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_with_search_term.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_with_search_term = document_term_matrix_df.append(random_search_term_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dtm_with_search_term.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_with_search_term.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df = pd.DataFrame(svd_matrix, \n",
    "                      index=dtm_with_search_term.index, \n",
    "                      columns=component_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_with_search_term[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Vector for our Search Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svd_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ff244e7ffe9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch_term_svd_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_search_term_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msearch_term_svd_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svd_df' is not defined"
     ]
    }
   ],
   "source": [
    "search_term_svd_vector = svd_df.loc[random_search_term_df.index]\n",
    "search_term_svd_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cosine Similarity to Find the Most Similar Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svd_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1a12ed192071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'svd_df' is not defined"
     ]
    }
   ],
   "source": [
    "svd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df['cosine_sim'] = cosine_similarity(svd_df, search_term_svd_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df[['cosine_sim']].sort_values('cosine_sim', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svd_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9f9cdc9f52c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtopic_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_word_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svd_array' is not defined"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=3, n_iter=1000, random_state=1)\n",
    "\n",
    "model.fit(X_)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 3\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
